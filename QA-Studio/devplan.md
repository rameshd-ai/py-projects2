## üéØ Project Goal

Build a **Python-only QA Studio** - a web dashboard for automated quality assurance testing of web properties. The Studio provides real-time test execution, comprehensive reporting, and validation across 6 core testing pillars. Users can configure test runs through a web UI, monitor progress via WebSockets, and review detailed results including screenshots, logs, and validation reports.

**Tech Foundation:** **Flask** for the UI, **Flask-SocketIO** for real-time progress, and **Native Playwright** for the automation engine.

---

## üõ†Ô∏è The Tech Stack (100% Python)

| Category | Tool | Why? |
| --- | --- | --- |
| **Backend** | **Flask** | Lightweight and perfect for a "Studio" dashboard. |
| **Real-time UI** | **Flask-SocketIO** | Streams logs and progress bars from the tests to the browser. |
| **Automation** | **Playwright (Python)** | Native browser control; handles Chromium, Firefox, and WebKit. |
| **Test Runner** | **Pytest** | Standard Python testing; handles parallel runs and reporting. |
| **Data/SEO** | **Pydantic + BeautifulSoup4** | Validates Schema.org JSON-LD and SEO meta-tags. |
| **Crawl Logic** | **Advertools** | High-speed sitemap parsing and broken link detection. |
| **Config Management** | **Pydantic + YAML** | Validates and manages test configuration (URLs, browsers, devices, SEO rules). |

---

## ‚öôÔ∏è Configuration & Input Model

**Purpose:** Define how users configure test runs and what settings are available.

**Implementation:**
- **Primary Method:** Web UI form in `templates/index.html` where users input:
  - Base URL (required)
  - Sitemap URL (optional, defaults to `{base_url}/sitemap.xml`)
  - Selected browsers (checkboxes: Chromium, Firefox, WebKit)
  - Selected devices/viewports (checkboxes: Desktop, Tablet, Mobile)
  - Selected test pillars (checkboxes for Pillars 1-6)
  - SEO validation rules (optional thresholds for H1 count, meta description length, etc.)

- **Config Storage:** 
  - Runtime config stored in memory (Pydantic models) per active run
  - Optional: Save configs to `configs/` directory as YAML for reuse
  - Pydantic models in `utils/config_models.py` validate all inputs

**Key Models:**
```python
# utils/config_models.py
- TestRunConfig (base_url, sitemap_url, browsers[], devices[], pillars[])
- SEOConfig (max_h1_count, min_meta_length, require_alt_tags, etc.)
- DeviceProfile (name, viewport_width, viewport_height, user_agent)
```

---

## üíæ Result Storage & Reports

**Purpose:** Persist test run results for review, history, and detailed analysis.

**Storage Strategy:**
- **File-based storage** in `static/reports/` directory:
  - Each run gets a unique `run_id` (timestamp-based: `YYYYMMDD_HHMMSS`)
  - Directory structure: `static/reports/{run_id}/`
    - `summary.json` - Overall run status, timestamps, pillar results
    - `screenshots/` - Visual regression images (baseline, actual, diff)
    - `logs/` - Per-pillar log files
    - `errors.json` - Aggregated errors, console messages, failed requests

**Data Model:**
```python
# utils/report_models.py
- RunSummary (run_id, timestamp, status, duration, config_snapshot)
- PillarResult (pillar_name, status, errors[], warnings[], metrics{})
- ErrorEntry (type, message, url, timestamp, stack_trace)
```

**UI Integration:**
- Dashboard shows recent runs in a table (run_id, timestamp, status, quick actions)
- Clicking a run opens detailed view: pillar-by-pillar breakdown, screenshots gallery, error logs
- Real-time runs also stream to this storage as they execute

---

## üñ•Ô∏è UI Flows for the Dashboard

**Purpose:** Define the user experience and what components are needed in `templates/index.html`.

**User Flow:**
1. **Landing/Configuration Page:**
   - Form with fields: Base URL, Sitemap URL, Browser checkboxes, Device checkboxes, Pillar checkboxes
   - "Run Tests" button (disabled if no URL/pillars selected)
   - Optional: "Load Saved Config" dropdown

2. **Active Run View:**
   - **Progress Section:** 6 progress bars (one per pillar) showing `pending ‚Üí running ‚Üí success/fail`
   - **Live Log Panel:** Real-time terminal output streamed via SocketIO (auto-scroll, color-coded by log level)
   - **Status Indicators:** Overall status badge (Running/Completed/Failed) with elapsed time
   - **Cancel Button:** Stops current run gracefully

3. **Results View (after completion):**
   - **Summary Card:** Overall pass/fail, duration, errors count
   - **Pillar Breakdown:** Expandable cards per pillar showing:
     - Status icon (‚úì/‚úó/‚ö†)
     - Error count, screenshot count
     - "View Details" link
   - **Screenshots Gallery:** Thumbnail grid of visual diffs (click to expand)
   - **Error Log:** Filterable table of all errors (type, message, URL, timestamp)
   - **Download Report:** Button to export full JSON report

4. **Run History Sidebar:**
   - List of recent runs (last 10-20) with quick status indicators
   - Click to load previous run's results

**Required UI Components:**
- Form inputs (text, checkboxes, dropdowns)
- Progress bars with status colors
- Terminal-style log viewer (monospace font, dark theme)
- Modal/overlay for screenshot viewer
- Data tables for errors and history
- Status badges and icons

---

## üèóÔ∏è The 6 Pillars of Your QA Studio

### 1. Rendering & Responsiveness

* **Logic:** Use Playwright‚Äôs device emulation (e.g., `iPhone 15`, `Desktop Chrome HiDPI`).
* **Visual Check:** Use Playwright's native `expect(page).to_have_screenshot()` for visual regression.

### 2. Site Architecture & Navigation

* **Logic:** Fetch the `sitemap.xml` using `advertools`.
* **Check:** Use a background thread to "ping" every URL to ensure `200 OK` status and no "orphan" pages.

### 3. Functional & Business Logic

* **Logic:** Use the **Page Object Model (POM)**.
* **Flows:** Define critical paths (Login, Checkout, Form Submission) as Python classes.

### 4. Console & Technical Checks

* **Logic:** Attach event listeners to the Playwright page object:
* `page.on("console", ...)` for JS errors.
* `page.on("requestfailed", ...)` for 404/500 assets.



### 5. Cross-Browser & Device Testing

* **Logic:** Configure `pytest.ini` or `conftest.py` to run the same test suite across three browsers simultaneously using `pytest-xdist`.

### 6. Content, SEO & Schema Validation

* **SEO:** Audit `<h1>` counts, meta-descriptions, and `alt` tags via BeautifulSoup.
* **Schema:** Extract JSON-LD and validate it against a **Pydantic Model** representing Schema.org standards.

---
This is the final, streamlined **Pillar-by-Pillar Development Plan** for your QA Studio. Each phase is designed to be a standalone module that you can feed into **Cursor**.

---
Phase 0: The Core Engine (The Dashboard)
Goal: Build the Flask server and the "Live Stream" terminal using WebSockets.

Tech: Flask, Flask-SocketIO, eventlet.

Key Feature: A background thread that executes Shell commands (like pytest) and streams the real-time output (STDOUT) to the UI.

**Concurrency & Execution Model:**
- **Single Run at a Time:** Only one test run can execute simultaneously (prevents resource conflicts, simplifies state management)
- **Background Execution:** Use Python's `threading.Thread` or `concurrent.futures.ThreadPoolExecutor` to run pytest in a non-blocking manner
- **SocketIO Rooms:** Each run gets a unique `run_id` and joins a SocketIO room (`f"run_{run_id}"`) for isolated log streaming
- **State Management:** Global state object tracks current run (or `None` if idle):
  ```python
  current_run = {
      "run_id": str,
      "status": "running" | "completed" | "failed" | "cancelled",
      "thread": Thread,
      "config": TestRunConfig
  }
  ```
- **Graceful Cancellation:** Implement `threading.Event` to signal cancellation, pytest processes check this flag and exit cleanly
- **Resource Cleanup:** Ensure Playwright browsers are closed even if run is cancelled or crashes

Phase 1: Rendering & Responsiveness (Pillar 1)
Goal: Multi-viewport validation and Visual Regression.

Tech: playwright (Native), Pillow.

Key Feature: Automated screenshots for Desktop, Tablet, and Mobile. Comparison logic using Playwright's native expect(page).to_have_screenshot() to detect pixel shifts.

**Implementation:** `tests/test_ui_responsiveness.py` - Viewports & Visuals

Phase 2: Site Architecture & Navigation (Pillar 2)
Goal: Sitemap validation and Link integrity.

Tech: advertools.

Key Feature: Compare the sitemap.xml against a live crawl. Identify 404 errors and "Orphan Pages" (live pages not found in the sitemap).

**Implementation:** `tests/test_site_structure.py` - Sitemap & 404 Checks

Phase 3: Functional & Business Logic (Pillar 3)
Goal: Critical path testing (Forms, CTAs, Widgets).

Tech: playwright (Native) + Page Object Model (POM).

Key Feature: Scripted flows for high-value actions like Form Submissions, Search widgets, and booking flows.

**Implementation:** `tests/test_user_flows.py` - Functional/E2E with `tests/page_objects/` classes

Phase 4: Technical & Console Audit (Pillar 4)
Goal: Catch silent errors and broken dependencies.

Tech: Playwright Browser Events.

Key Feature: Listeners for page.on("console") and page.on("pageerror"). Dashboard logs for JavaScript crashes and failed network requests (404/500 assets).

**Implementation:** `tests/test_browser_health.py` - Console & Network Errors

Phase 5: Cross-Browser Execution (Pillar 5)
Goal: Environment compatibility.

Tech: pytest-playwright.

Key Feature: Configure the engine to toggle between Chromium, Firefox, and WebKit engines directly from the Flask UI.

**Implementation:** `tests/test_compatibility.py` - Multi-engine Config

Phase 6: Content, SEO & Schema Validation (Pillar 6)
Goal: Search engine readiness and data accuracy.

Tech: BeautifulSoup4, Pydantic.

Key Feature: Extract and validate JSON-LD Schema against Pydantic models; audit meta tags, H1-H6 hierarchy, and image alt text.

**Implementation:** `tests/test_seo_metadata.py` - SEO & Schema

---

## üõ°Ô∏è Error Handling & Health Checks

**Purpose:** Ensure robust error handling across all phases and clear communication of failures to the UI.

**Error Classification:**
- **Critical Errors:** Test run cannot continue (e.g., invalid URL, Playwright installation missing, pytest crash)
- **Pillar Failures:** Specific pillar encountered errors but run continues (e.g., 404s found, visual diffs detected, SEO violations)
- **Warnings:** Non-blocking issues (e.g., missing alt tags, slow page load, console warnings)

**Error Response Model:**
```python
# utils/error_models.py
- ErrorResponse (
    phase: str,              # "ui_responsiveness", "site_structure", etc.
    status: str,             # "pending" | "running" | "success" | "failed" | "warning"
    message: str,            # Human-readable summary
    errors: List[ErrorEntry], # Detailed error list
    warnings: List[str],     # Non-critical issues
    metrics: dict           # Pillar-specific metrics (e.g., {"screenshots_taken": 5})
)
```

**Implementation Strategy:**
- **Try-Catch Wrappers:** Each pillar's test execution wrapped in try-catch to prevent one failure from crashing entire run
- **Status Propagation:** Each pillar emits status updates via SocketIO: `emit('pillar_status', {'pillar': 'ui_responsiveness', 'status': 'failed', 'message': '...'})`
- **Graceful Degradation:** If a pillar fails, mark it as failed but continue with remaining pillars
- **Health Checks:**
  - Validate Playwright installation on startup
  - Check network connectivity to base URL before starting
  - Verify required directories exist (`static/reports/`, etc.)
  - Monitor thread health (detect hung processes)

**UI Error Display:**
- Real-time error badges on progress bars (red for failed, yellow for warnings)
- Expandable error details in log panel
- Summary error count in header
- Failed pillar cards show error count and "View Errors" button

---

## üß™ Testing the Tester (Meta-Testing)

**Purpose:** Ensure the QA Studio itself is reliable through unit tests and smoke tests.

**Test Coverage:**
- **Unit Tests** (`tests/unit/`):
  - `test_seo_parser.py` - Validate BeautifulSoup parsing, Pydantic schema validation
  - `test_sitemap_handler.py` - Test advertools integration, URL validation
  - `test_image_processor.py` - Test Pillow diffing logic, screenshot comparison
  - `test_config_models.py` - Test Pydantic config validation, edge cases

- **Integration Tests** (`tests/integration/`):
  - `test_app_routes.py` - Test Flask routes (GET `/`, POST `/run`, etc.)
  - `test_socketio_events.py` - Test WebSocket events (connect, disconnect, log streaming)
  - `test_background_runner.py` - Test pytest execution in background thread

- **Smoke Tests** (`tests/smoke/`):
  - `test_studio_startup.py` - Verify Flask server starts, all dependencies available
  - `test_minimal_run.py` - Execute a minimal test run against a test website (e.g., httpbin.org)

**Test Execution:**
- Run meta-tests with: `pytest tests/unit tests/integration tests/smoke -v`
- Keep meta-tests fast (< 30 seconds total) to encourage frequent runs
- Use pytest fixtures for shared test data (mock configs, test HTML files)

---

That makes perfect sense. Using descriptive, professional names rather than "Pillar 1, 2, etc." makes the codebase much more maintainable and easier for Cursor (or any developer) to navigate. Here is your updated project structure with logical, industry-standard naming conventions.

## üìÇ Final Project Structure

```
QA-STUDIO/
‚îú‚îÄ‚îÄ app.py                      # Flask Server & SocketIO Manager
‚îú‚îÄ‚îÄ requirements.txt            # Project Dependencies
‚îú‚îÄ‚îÄ configs/                    # Saved test configurations (YAML)
‚îÇ   ‚îî‚îÄ‚îÄ example_config.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html              # Dashboard UI
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ css/                    # Tailwind / Custom Styles
‚îÇ   ‚îú‚îÄ‚îÄ js/                     # SocketIO Client Logic
‚îÇ   ‚îî‚îÄ‚îÄ reports/                # Test run results & screenshots
‚îÇ       ‚îî‚îÄ‚îÄ {run_id}/           # Per-run directories
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ config_models.py        # Pydantic models for test configuration
‚îÇ   ‚îú‚îÄ‚îÄ report_models.py        # Pydantic models for run results
‚îÇ   ‚îú‚îÄ‚îÄ error_models.py         # Error classification & response models
‚îÇ   ‚îú‚îÄ‚îÄ image_processor.py      # Image diffing & Pillow logic
‚îÇ   ‚îú‚îÄ‚îÄ seo_parser.py           # BeautifulSoup & Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ sitemap_handler.py      # Advertools integration
‚îÇ   ‚îî‚îÄ‚îÄ background_runner.py    # Thread execution & pytest runner
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ conftest.py             # Global Playwright Configuration
    ‚îú‚îÄ‚îÄ test_ui_responsiveness.py # (Pillar 1) Viewports & Visuals
    ‚îú‚îÄ‚îÄ test_site_structure.py   # (Pillar 2) Sitemap & 404 Checks
    ‚îú‚îÄ‚îÄ test_user_flows.py       # (Pillar 3) Functional/E2E
    ‚îú‚îÄ‚îÄ test_browser_health.py   # (Pillar 4) Console & Network Errors
    ‚îú‚îÄ‚îÄ test_compatibility.py    # (Pillar 5) Multi-engine Config
    ‚îú‚îÄ‚îÄ test_seo_metadata.py     # (Pillar 6) SEO & Schema
    ‚îú‚îÄ‚îÄ page_objects/           # POM classes for user flows
    ‚îú‚îÄ‚îÄ unit/                   # Unit tests for utils/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_seo_parser.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_sitemap_handler.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_image_processor.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_config_models.py
    ‚îú‚îÄ‚îÄ integration/            # Integration tests
    ‚îÇ   ‚îú‚îÄ‚îÄ test_app_routes.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_socketio_events.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_background_runner.py
    ‚îî‚îÄ‚îÄ smoke/                  # Smoke tests
        ‚îú‚îÄ‚îÄ test_studio_startup.py
        ‚îî‚îÄ‚îÄ test_minimal_run.py
```


